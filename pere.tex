\documentclass[11pt, aspectratio=169]{beamer}

% Packages for math, language, encoding etc.
\usepackage{tikz, ctable}
\usepackage{bm}
\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[absolute,overlay]{textpos}	
\usepackage{amsfonts,amssymb,amsbsy,amsthm,amsmath,mathtools,enumerate,verbatim}
\usepackage{stmaryrd} % double square brackets
\usepackage{color, colortbl}
\usepackage[font=scriptsize]{caption}
\usepackage{csquotes}
\usepackage{tabularray}
\usepackage{hyperref}
\usepackage{datetime}

\usepackage[T1]{fontenc}
\usepackage[tracking=smallcaps, letterspace=-55]{microtype} % package for font spacing

\author[Jaakko Pere]{Jaakko Pere}

\title{On the Impact of Approximation Errors on Extreme Quantile Estimation with
Applications to Functional Data Analysis}

\subtitle{Based on collaboration with Pauliina Ilmonen, Lauri Viitasaari,
Valentin Garino and Benny Avelin \\
\url{https://doi.org/10.48550/arXiv.2307.03581} (submitted to a journal)}



\date{7th of May, 2025}

\institute{Dep.\ of Mathematics and Statistics, University of Helsinki}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% load packages
% add packages if needed

% set beamer colors
\definecolor{hyblue}{RGB}{0,155,255}
\definecolor{hyscience}{RGB}{252,163,17}

\setbeamercolor{alerted text}{fg=hyscience}
\setbeamercolor{structure}{fg=hyblue}
\setbeamercolor{item}{fg=white}
%\setbeamercolor{section in toc}{fg=white,bg=gray}

\setbeamercolor{background canvas}{bg=black!90}
\setbeamercolor{normal text}{fg=white}\usebeamercolor*{normal text}

%\setbeamertemplate{itemize item}{\color{white}\bullet}

% set font (helvetica plays the role of Arial)
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

% set frametitle
\setbeamercolor{frametitle}{fg=white}
\setbeamerfont{frametitle}{series=\bfseries, size=\large}
\setbeamertemplate{frametitle}[default][left,leftskip=1cm] % left shift of frame title
\addtobeamertemplate{frametitle}{\vspace{0.5cm}}{\vspace{1cm}} % spacing above and below frame title

% set footline
\beamertemplatenavigationsymbolsempty

% Other style settings
\setbeamertemplate{itemize items}[circle] % makes the bullets in lists circles


% Bibliography and style.
\usepackage[style=authoryear]{biblatex}
\addbibresource{sources.bib}
\setbeamertemplate{bibliography item}{}

% Show greyed table of contents before section.
\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}
\setcounter{tocdepth}{1}


\AtBeginSubsection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{subtitle}
    \usebeamerfont{subtitle}\insertsubsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


\DeclareMathOperator{\mda}{MDA}
\DeclareMathOperator{\mrv}{MRV}
\DeclareMathOperator{\rv}{RV}
\DeclareMathOperator{\erv}{ERV}

\begin{document}

% Title page
{
  \setbeamercolor{background canvas}{bg=black!90}
  \setbeamertemplate{background}{
    \setlength{\unitlength}{1cm}
    \begin{picture}(16,8)
      \put(-0.1,5){\includegraphics[width=3cm]{logo-white.png}}
    \end{picture}
    \setlength{\unitlength}{1pt}
  }
  %\setbeamertemplate{headline}{ }%
  \begin{frame}
    \vspace{2.5cm}
    \begin{center}
      \textcolor{hyblue}{\bf\MakeUppercase{\Large\inserttitle}} \\
      {\footnotesize\insertsubtitle} \\
      {\large\insertauthor} \\
      {\large\insertdate} \\
      {\large\insertinstitute}
    \end{center}
  \end{frame}
}

\begin{frame}{Agenda of the presentation}
  \tableofcontents
\end{frame}

\section{Univariate Extreme Value Theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What is Extreme Value Theory?}
  Extreme value theory is concerned about inference of rare events.
  \pause
  \vspace{\baselineskip}
  \begin{itemize}
    \item Extreme quantile estimation
    \item Tail probability estimation
    \item Estimation of the endpoint of a given distribution
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Maximum Domain of Attraction}
  \begin{definition}
    Let $Y_1, \ldots, Y_n$ be i.i.d.\ observations of a random variable $Y$. If
    there exist sequences $a_n > 0$ and $b_n\in\mathbb{R}$, and a random
    variable $G$ with a nondegenerate distribution such that
    \begin{equation*}
	    \frac{\max\left(Y_1, \ldots, Y_n\right) - b_n}{a_n}
      \stackrel{\mathcal{D}}{\to} G, \quad n\to\infty,
    \end{equation*}
    we say that $Y$ belongs to the maximum domain of attraction of $G$, and
    denote $Y\in\mda\left(G\right)$.
  \end{definition}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Extreme Value Index}
  \begin{theorem}[\cite{fisher1928,gnedenko1943}]
    Up to location and scale, the distribution of $G = G_\gamma$ is
    characterized by the parameter $\gamma$, called the \emph{extreme value
    index}. That is, the distribution of ${G_\gamma}$ is of the type
    \begin{equation*}
      F_{G_\gamma}\left(x\right) =
      \begin{cases}
        \exp\left(-\left(1 + \gamma x\right)^{-1/\gamma}\right),
        \quad 1 + \gamma x > 0 &\textnormal{if}\quad \gamma\neq 0, \\
        \exp\left(-e^{-x}\right),
        \quad x\in\mathbb{R} &\textnormal{if}\quad \gamma = 0.
      \end{cases}
    \end{equation*}
  \end{theorem}
  \pause
  In the case $\gamma > 0$ the type of $G_\gamma$ is Fr\'echet,
  \begin{equation*}
    \Phi_\gamma\left(x\right) =
    \begin{cases}
      0, & x\leq 0 \\
      \exp\left(-x^{-1/\gamma}\right), & x > 0.
    \end{cases}
  \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Tail Quantile Function}
  Define the tail quantile function corresponding to a distribution $F$ by
  \begin{equation*}
    U\left(t\right) = F^{\leftarrow}\left(1 - \frac{1}{t}\right), \quad t > 1,
  \end{equation*}
  where we denote the left-continuous inverse of a nondecreasing function by
  $f^\leftarrow\left(y\right) = \inf\left\{x\in\mathbb{R} : f(x) \geq
  y\right\}$.
  \pause
  \begin{itemize}
    \item[] 
  \end{itemize}
  That is, $U\left(1/p\right)$ is the $(1-p)$-quantile.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \begin{definition}[Regular variation]
    A Lebesgue measurable function $f:\mathbb{R}^+\to\mathbb{R}$ that is
    eventually positive is regularly varying with index $\alpha\in\mathbb{R}$ if
    for all $x > 0$,
    \begin{equation*}
      \lim_{t\to\infty}\frac{f\left(tx\right)}{f\left(t\right)} = x^\alpha.
    \end{equation*}
    Then we denote $f\in \rv_\alpha$. Furthermore, we say that a function $f$ is
    slowly varying if $f\in \rv_0$.
  \end{definition}
  \pause
  Intuition:
  \begin{equation*}
    f\in\rv_\alpha \iff f(x) = L(x)x^{\alpha}, \quad L\in\rv_0.
  \end{equation*}
  We also have
  \begin{equation*}
    \lim_{x\to\infty} x^{-\varepsilon} L(x) = 0, \quad\forall
    \, \varepsilon > 0.
  \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Construction of an Extreme Quantile Estimator}
  \begin{theorem}[\parencite{gnedenko1943,dehaan1970}]
    Let $\gamma > 0$. We have
    \begin{equation*}
      Y\in\mda\left(G_\gamma\right) \iff 1-F\in \rv_{-1/\gamma}
      \iff U\in \rv_\gamma.
    \end{equation*}
    \pause
  \end{theorem}
  Choose $t=n/k$ and $x = k/(np)$ to get the approximation
    \begin{equation*}
      U\left(\frac{1}{p}\right)\approx U\left(\frac{n}{k}\right)
      \left(\frac{k}{np}\right)^{\gamma}.
    \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Extreme Quantile Estimation}
  Suppose $\bm Y = \left(Y_1, \ldots, Y_n\right)$ is an i.i.d.\ sample of
  $Y\in\mda\left(G_\gamma\right)$, $\gamma > 0$. Denote order statistics
  corresponding to the sample $\bm Y$ by $\bm Y_{1, n} \leq \dots \leq \bm Y_{n,
  n}$. Then an estimator for the extreme $(1-p)$-quantile $x_{p} =
  U\left(1/p\right)$ can be given as
  \begin{equation*}
    \hat x_{p}\left(\bm Y\right) = \bm Y_{n-k,n}
    \left(\frac{k}{np}\right)^{\hat\gamma\left(\bm Y\right)},
  \end{equation*}
  where $\hat\gamma$ is an estimator for the extreme value index $\gamma$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The Hill Estimator \parencite{hill1975,mason1982}}
  Suppose $\bm Y = \left(Y_1, \ldots, Y_n\right)$ is an i.i.d.\ sample of
  $Y\in\mda\left(G_\gamma\right)$, $\gamma > 0$. The Hill estimator is defined
  as
  \begin{equation*}
    \hat\gamma_H\left(\bm Y\right) = \frac{1}{k}\sum_{i = 0}^{k - 1}
    \ln\left(\frac{\bm Y_{n-i,n}}{\bm Y_{n-k,n}}\right).
  \end{equation*}
  \pause
  If additionally as $n\to\infty$, $k = k_n\to\infty$, $k/n\to 0$, then
  \begin{equation*}
    \hat\gamma_H\left(\bm Y\right)\stackrel{\mathbb{P}}{\to} \gamma,
    \quad n\to\infty.
  \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Impact of Approximation Errors}

\begin{frame}{The General Framework}
  \begin{itemize}
    \item What if instead of the sample $\bm Y$, only approximations $\hat{\bm
    Y} = (\hat Y_1, \ldots, \hat Y_n)$ are available?
    \pause
    \item How the approximation error affects the asymptotics?
    \pause
    \item $\Rightarrow$ Useful approach in multivariate and infinite dimensional
    settings:
    \begin{itemize}
      \item Let $X\in \mathbb{S}$ be a random object, where, e.g., $\mathbb{S} =
      \mathbb{R}^d$ or $\mathbb{S} = L^p([0,1]^d)$.
      \item Let $g:\mathbb{S}\to\mathbb{R}$ be some suitable map.
      \item Apply extreme value theory to $g(X)$.
    \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Approximated $L^p$-Norms}
  \begin{itemize}
    \item Let $X\in L^p\left([0, 1]^d\right)$, and let $X_1,\ldots, X_n$ be
    i.i.d.\ copies of $X$.
    \item We wish to estimate extreme value index and extreme quantiles
    corresponding to $\|X\|_p\in\mda\left(G_\gamma\right)$, $\gamma > 0$.
    \pause
    \item In practice we never observe $X_1, \ldots, X_n$.
    \pause
    \item Approximate norms with Riemann sums or Monte Carlo integration.
    \item Use approximated norms $\hat{Y}_i$ in the estimation.
    \pause
    \item As the estimator of the extreme value index we choose the Hill
    estimator
    \begin{equation*}
      \hat\gamma(\hat{\bm Y}) = \frac{1}{k}\sum_{i = 0}^{k-1}
      \ln\left(\frac{\hat{\bm Y}_{n-i,n}}{\hat{\bm Y}_{n-k,n}}\right).
    \end{equation*}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Draft of the Main Result}
  Let $\gamma > 0$. Let $Y_1, \ldots, Y_n$ be i.i.d.\ copies of
  $Y\in\mda\left(G_\gamma\right)$ and $\hat{\bm Y} = (\hat Y_1, \ldots, \hat
  Y_n)$ the corresponding approximations. Denote errors by $E_i = \left|\hat Y_i
  - Y_i\right|$. If
  \begin{equation*}
    \sqrt{k}\frac{\bm E_{n,n}}{U_Y\left(n/k\right)}\stackrel{\mathbb{P}}{\to}0,
    \quad n\to\infty,
  \end{equation*}
  then
  \begin{equation*}
    \sqrt{k}\left(\hat\gamma(\hat{\bm Y}) - \gamma\right)
    \quad\textnormal{and}\quad
    \frac{\sqrt{k}}{\ln\left(k/(np)\right)}\left(\frac{\hat x_p(\hat{\bm Y})}
    {U(1/p)} - 1\right)
  \end{equation*}
  are asymptotically normally distributed under the standard assumptions
  (second-order condition, rate for $p = p_n$, $k = k_n\to\infty$, $k/n\to 0$,
  as $n\to\infty$). 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extreme Quantile Estimation for $L^p$-Norms}

\begin{frame}{Riemann Sum Approximated Norms}
  Let $\gamma > 0$. Let $X_i$ be i.i.d.\ copies of $X\in L^p\left([0,1]\right)$,
  $p\in [1,\infty]$, s.t. $Y = \|X\|_p\in\mda\left(G_\gamma\right)$. Let $\hat
  Y_i$ be the Riemann sum approximated norms (based on discretizations with $m$
  equidistant observed points). Suppose for all $s,t\in [0,1]$, $X$ satisfies
  \begin{equation*}
    \left|X(t) - X(s)\right| \leq V\phi\left(|t-s|\right) \quad a.s.,
  \end{equation*}
  for some random variable $V\in\mda\left(G_{\gamma'}\right)$, $\gamma' > 0$,
  and for some continuous decreasing function $\phi:\mathbb{R}^+\to
  \mathbb{R}^+$ with $\phi\left(0\right) = 0$. Then the condition
  \begin{equation*}
    \sqrt{k}\frac{\bm E_{n,n}}{U_Y\left(n/k\right)}\stackrel{\mathbb{P}}{\to}0,
    \quad n\to\infty,
  \end{equation*}
  translates into
  \begin{equation*}
    \sqrt{k}\phi\left(\frac{1}{m}\right)k^\gamma n^{\gamma' - \gamma}\to 0,
    \quad n\to\infty.
  \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Concentration for $\hat\gamma(\hat{\bm Y})$} In order to give
  concentration inequality for $\mathbb{P}\left(\left|\hat\gamma(\hat{\bm Y}) -
  \hat\gamma(\bm Y)\right| > x\right)$ one needs to control the errors
  \begin{equation*}
    \mathbb{P}\left(\frac{\bm E_{n,n}}{U_Y\left(n/k\right)} > x\right)
  \end{equation*}
  and the convergence
  \begin{equation*}
    \mathbb{P}\left(\left|\frac{\bm
    Y_{n-k,n}}{U(n/k)} - 1\right| > x\right).
  \end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Chernoff-Type Bound for Intermediate Order Statistics}
  Let $\gamma > 0$. Let $\bm Y = \left(Y_1, \ldots, Y_n\right)$ be an i.i.d.\
  sample of $Y\in\mda\left(G_\gamma\right)$ and assume that, as $n\to\infty$,
  $k=k_n\to\infty$, and $k/n\to 0$. Then for sufficiently large $n$
  \begin{equation*}
    \mathbb{P}\left(\left|\frac{\bm
    Y_{n-k,n}}{U(n/k)} - 1\right| > x\right)
    \leq C_1 e^{-C_2 k},
  \end{equation*} 
  where the constants $C_1 > 0$ and $C_2 > 0$ depend on $x$ and $\gamma$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  Thank you for your attention!
  \begin{itemize}
    \item Link to the manuscript (arXiv): \\
    \url{https://doi.org/10.48550/arXiv.2307.03581}
    \item Link to slides (Github): \\
    \url{https://github.com/perej1/ics-and-related}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]{References}
  \printbibliography
\end{frame}

\end{document}





